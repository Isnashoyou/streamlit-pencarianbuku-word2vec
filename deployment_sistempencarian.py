# -*- coding: utf-8 -*-
"""deployment_skripsi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pCCVcjlDyPb_ndCRDc7MZTC47uT2X1lL
"""

import streamlit as st
st.write("App loaded!")
import pandas as pd
import numpy as np
import ast
import re
from sklearn.metrics.pairwise import cosine_similarity
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

nltk.download('punkt_tab')


# === Konfigurasi Awal Streamlit ===
st.set_page_config(page_title="Sistem Pencarian Buku", layout="wide")

# === Stopwords dan Standarisasi ===
sastrawi_stopwords = StopWordRemoverFactory().get_stop_words()

with open("id.stopwords.02.01.2016.txt", encoding="utf-8") as f:
    extra_stopwords = f.read().splitlines()

with open("stopword_custom.txt", encoding="utf-8") as c:
    custom_stopwords = c.read().splitlines()

all_stopwords = set(sastrawi_stopwords + extra_stopwords + custom_stopwords)

standarisasi_kata = {
    'budi daya': 'budidaya',
    'basis data': 'database',
    'chat bot' : 'chatbot',
    'statistik': 'statistika',
    'type': 'tipe',
    'kecerdasan buatan': 'artificial intelligence',
    'pembelajaran mesin': 'machine learning',
}

def preprocess_text(text):
    text = text.lower()
    text = text.replace('-', ' ')
    text = text.replace('\n', ' ').replace('\t', ' ')
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\b(?:bab\s*\d+|\d+\s*bab)\b', '', text)
    text = re.sub(r'\b\d+\b', '', text)
    text = text.strip()

    for bentuk_lama, bentuk_baru in standarisasi_kata.items():
        text = re.sub(rf'\b{re.escape(bentuk_lama)}\b', bentuk_baru, text)

    tokens = word_tokenize(text)
    filtered_tokens = [word for word in tokens if word not in all_stopwords]
    return filtered_tokens

# === Load Data dan Model ===
@st.cache_data
def load_data():
    data = pd.read_excel("data_buku_preprocessed2.xlsx")
    data['tokens'] = data['tokens'].apply(ast.literal_eval)
    data['tokens_judul'] = data['tokens_judul'].apply(ast.literal_eval)
    return data

@st.cache_resource
def load_model():
    return Word2Vec.load("best_model_ns2.model")

data = load_data()
best_model_ns = load_model()

# === Fungsi Pencarian Buku ===
def search_books(query, model, df, threshold=0.70, alpha=0.65):
    query_tokens = preprocess_text(query)
    query_vectors = [model.wv[word] for word in query_tokens if word in model.wv]
    if not query_vectors:
        return pd.DataFrame()

    query_mean = np.mean(query_vectors, axis=0).reshape(1, -1)
    similarities = []

    for _, row in df.iterrows():
            # Judul
            judul_tokens = row['tokens_judul']
            judul_vecs = [model.wv[word] for word in judul_tokens if word in model.wv]
            sim_judul = cosine_similarity(query_mean, np.mean(judul_vecs, axis=0).reshape(1, -1))[0][0] if judul_vecs else 0
    
            # Deskripsi
            desk_tokens = row['tokens']
            desk_vecs = [model.wv[word] for word in desk_tokens if word in model.wv]
            sim_desk = cosine_similarity(query_mean, np.mean(desk_vecs, axis=0).reshape(1, -1))[0][0] if desk_vecs else 0
    
            # Gabungan skor (judul lebih berat)
            sim_total = alpha * sim_judul + (1 - alpha) * sim_desk
            similarities.append(sim_total)

    df['similarity'] = similarities
    results = df[df['similarity'] >= threshold].sort_values(by='similarity', ascending=False)
    return results

# === UI Streamlit ===
st.title("ðŸ“šSistem Pencarian Buku\nDEPLOYMENT")
query = st.text_input("Masukkan kata kunci pencarian:")

if query:
    results = search_books(query, best_model_ns, data)

    if isinstance(results, str) or results.empty:
        st.warning("Buku tidak ditemukan.")
    else:
        filter_mode = st.radio("Tampilkan", ["10 Teratas", "Semua"], horizontal=True)
        if filter_mode == "10 Teratas":
            results = results.head(10)

        for _, row in results.iterrows():
            with st.container():
                col1, col2 = st.columns([1, 6])

                with col1:
                    st.image(row['cover'], width=130)

                with col2:
                    st.markdown(f"**{row['judul']}**")
                    with st.expander("Lihat Deskripsi"):
                        st.write(row['deskripsi'])
                    st.caption(f"Cosine Similarity: {row['similarity']:.2f}")

            st.markdown("---")
